# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QMQvfPcvZqlM8sX46CF1z9L2NWkEl-lt
"""

# Step 1: Install necessary library if not already installed
!pip install liac-arff

# Step 2: Imports
import pandas as pd
import numpy as np
import arff
from google.colab import files
import io # Import io

# Step 3: Upload the .arff file
uploaded = files.upload()

# Step 4: Get the uploaded filename and load ARFF file
# Get the first (and likely only) filename from the uploaded dictionary keys
file_name = list(uploaded.keys())[0]

# Load the ARFF data using the correct filename
arff_data = arff.load(io.StringIO(uploaded[file_name].decode('utf-8')))
df = pd.DataFrame(arff_data['data'], columns=[attr[0] for attr in arff_data['attributes']])

# Step 5: Replace '?' with NaN
df.replace('?', np.nan, inplace=True)

# Step 6: Convert appropriate columns to numeric
# Note: Adjusted numeric_cols based on common CKD datasets. You might need to verify these.
numeric_cols = ['age', 'bp', 'sg', 'al', 'su', 'bgr', 'bu', 'sc', 'sod', 'pot',
                'hemo', 'pcv', 'wc', 'rc'] # Assuming wc and rc are 'wbcc' and 'rbcc' from the previous cell

for col in numeric_cols:
    # Use a check to ensure the column exists before trying to convert
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')  # 'errors=coerce' turns bad values into NaN
    else:
        print(f"Warning: Column '{col}' not found in DataFrame.")


# Step 7: Display how many missing values each column has
print("\nMissing values per column:\n")
print(df.isnull().sum().sort_values(ascending=False))

# Step 8 (optional): View the cleaned data
pd.set_option('display.max_columns', None)
df.head()

print("Missing values per column:\n")
print(df.isnull().sum().sort_values(ascending=False))

from sklearn.impute import KNNImputer

# Convert all data to numeric (you must encode categorical values first)
from sklearn.preprocessing import LabelEncoder

df_encoded = df.copy()
for col in df_encoded.select_dtypes(include=['object']).columns:
    df_encoded[col] = LabelEncoder().fit_transform(df_encoded[col].astype(str))

# Now use KNN to impute
knn_imputer = KNNImputer(n_neighbors=5)
df_imputed = pd.DataFrame(knn_imputer.fit_transform(df_encoded), columns=df.columns)

# Optional: Convert label columns back if needed

print(df.isnull().sum().sum())  # Should be 0 if all missing values are imputed

from sklearn.impute import SimpleImputer

# Separate numeric and categorical columns
numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns
categorical_cols = df.select_dtypes(include=['object']).columns

# Impute numeric columns with mean
num_imputer = SimpleImputer(strategy='mean')
df[numeric_cols] = num_imputer.fit_transform(df[numeric_cols])

# Impute categorical columns with mode
cat_imputer = SimpleImputer(strategy='most_frequent')
df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])

print(df.isnull().sum().sum())  # Should be 0 if all missing values are imputed

from sklearn.impute import SimpleImputer

# Split columns into numerical and categorical
num_cols = df.select_dtypes(include=['float64', 'int64']).columns
cat_cols = df.select_dtypes(include='object').columns

# Impute numerical with median
num_imputer = SimpleImputer(strategy='median')
df[num_cols] = num_imputer.fit_transform(df[num_cols])

# Impute categorical with most frequent
cat_imputer = SimpleImputer(strategy='most_frequent')
df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])

print("Missing values after imputation:\n")
print(df.isnull().sum().sum())  # Should be 0 now

import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer

# Step 1: Replace '?' with NaN
df.replace('?', np.nan, inplace=True)

# Step 2: Convert numeric columns to correct types
numeric_columns = ['age', 'bp', 'bgr', 'bu', 'sc', 'sod', 'pot',
                   'hemo', 'pcv', 'wbcc', 'rbcc']

for col in numeric_columns:
    df[col] = pd.to_numeric(df[col], errors='coerce')

# Step 3: Check missing values
print("Missing values BEFORE imputation:\n")
print(df.isnull().sum().sort_values(ascending=False))

# Step 4: Impute numerical with median
num_cols = df.select_dtypes(include=['float64', 'int64']).columns
num_imputer = SimpleImputer(strategy='median')
df[num_cols] = num_imputer.fit_transform(df[num_cols])

# Step 5: Impute categorical with mode
cat_cols = df.select_dtypes(include='object').columns
cat_imputer = SimpleImputer(strategy='most_frequent')
df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])

# Step 6: Final check
print("\nMissing values AFTER imputation:\n")
print(df.isnull().sum().sum())  # Should now be 0

from sklearn.impute import SimpleImputer
import numpy as np
import pandas as pd

# Step 1: Replace '?' with NaN
df.replace('?', np.nan, inplace=True)

# Step 2: Convert numeric columns
numeric_columns = ['age', 'bp', 'bgr', 'bu', 'sc', 'sod', 'pot',
                   'hemo', 'pcv', 'wbcc', 'rbcc']
for col in numeric_columns:
    df[col] = pd.to_numeric(df[col], errors='coerce')

# Step 3: Convert object columns explicitly to string (important!)
cat_cols = df.select_dtypes(include='object').columns
df[cat_cols] = df[cat_cols].astype(str)
df[cat_cols] = df[cat_cols].replace('nan', np.nan)  # Handle 'nan' strings

# Step 4: Impute numerics with median
num_imputer = SimpleImputer(strategy='median')
df[numeric_columns] = num_imputer.fit_transform(df[numeric_columns])

# Step 5: Impute categoricals with mode
cat_imputer = SimpleImputer(strategy='most_frequent')
df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])

# Step 6: Final check
print("Missing values after imputation:", df.isnull().sum().sum())

print("Missing values after imputation:\n")
print(df.isnull().sum().sum())  # Should be 0 now

from sklearn.preprocessing import LabelEncoder

# Create a LabelEncoder object
le = LabelEncoder()

# Loop through all object (categorical) columns and encode them
for col in df.select_dtypes(include='object').columns:
    df[col] = le.fit_transform(df[col])

# Check the first few rows to see the result
df.head()

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Optional: Make plots look better
sns.set(style='whitegrid')
# %matplotlib inline

print("Class Distribution:")
print(df['class'].value_counts())
sns.countplot(data=df, x='class')
plt.title("CKD vs Not CKD Distribution")
plt.show()

numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns

for col in numeric_cols:
    plt.figure(figsize=(6, 3))
    sns.histplot(df[col], kde=True, bins=30)
    plt.title(f"Distribution of {col}")
    plt.xlabel(col)
    plt.ylabel("Count")
    plt.tight_layout()
    plt.show()

for col in numeric_cols:
    plt.figure(figsize=(6, 3))
    sns.boxplot(data=df, x='class', y=col)
    plt.title(f"{col} by CKD Class")
    plt.tight_layout()
    plt.show()

plt.figure(figsize=(12, 10))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=".2f", square=True)
plt.title("Feature Correlation Heatmap")
plt.show()

df.drop(columns=['pcv'], inplace=True)

from sklearn.ensemble import RandomForestClassifier

X = df.drop('class', axis=1)
y = df['class']

model = RandomForestClassifier(random_state=42)
model.fit(X, y)

importances = pd.Series(model.feature_importances_, index=X.columns)
importances = importances.sort_values(ascending=False)

# Visualize
plt.figure(figsize=(10, 6))
sns.barplot(x=importances[:10], y=importances.index[:10])
plt.title("Top 10 Most Important Features")
plt.show()

# Get full feature importance list
importances = pd.Series(model.feature_importances_, index=X.columns)

# Print features with importance less than 0.001
low_importance_features = importances[importances < 0.001]
print("Low-importance features (< 0.001):")
print(low_importance_features)

# Optionally drop them
df.drop(columns=low_importance_features.index, inplace=True)

import seaborn as sns
import matplotlib.pyplot as plt

# Only take numerical columns
numeric_df = df.select_dtypes(include=['float64', 'int64'])

plt.figure(figsize=(12, 10))
corr = numeric_df.corr()
sns.heatmap(corr, annot=True, cmap='coolwarm')

# Check pairs with high correlation
high_corr = corr[(corr > 0.85) & (corr < 1.0)]
print("Highly correlated pairs:\n", high_corr.dropna(how='all').dropna(axis=1, how='all'))

# Create a ratio of blood urea nitrogen (BUN) to creatinine
df['bun_sc_ratio'] = df['bu'] / df['sc']

# Flag high creatinine
df['high_creatinine'] = (df['sc'] > 1.2).astype(int)

X = df.drop('class', axis=1)
y = df['class']

# Convert 'al' and 'su' to numeric
for col in ['al', 'su']:
    df[col] = pd.to_numeric(df[col], errors='coerce')

# (Optional: Plot boxplots for exploration)
# Comment this out before saving model
# for feature in ['sc', 'bu', 'bgr']:
#     sns.boxplot(x=df[feature])
#     plt.title(f'Boxplot of {feature}')
#     plt.show()

# Feature interaction
df['hemo_bu'] = df['hemo'] * df['bu']  # Keep this if useful

# Optional: Drop rows with too many NaNs (depends on your preprocessing strategy)
df = df.fillna(0)  # or use median imputation

# Final checks (optional)
print("\nUpdated Data Types:\n", df.dtypes)
print("Final shape of the dataset:", df.shape)

print(df['class'].unique())

print(df['class'].isnull().sum())  # Should print 0

print(df['class'].unique())

print(df['class'].unique())       # Should be [0, 1]
print(df['class'].isnull().sum()) # Should be 0

# See what's in the class column (raw)
print("Before fix - class value counts:\n", df['class'].value_counts(dropna=False))
print("Data types:\n", df.dtypes)
print("Number of NaNs in 'class':", df['class'].isnull().sum())

print(df['class'].unique())       # Should be [0, 1]
print(df['class'].isnull().sum())

print("Columns in DataFrame:", df.columns.tolist())
print("First 10 rows of 'class':")
print(df['class'].head(10))
print("Number of missing values in 'class':", df['class'].isnull().sum())
print("Data type of 'class':", df['class'].dtype)

print(df['class'].unique())

from sklearn.model_selection import train_test_split

# Split features and labels
X = df.drop('class', axis=1)
y = df['class']

# Train-test split with stratification
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("Train shape:", X_train.shape)
print("Test shape:", X_test.shape)
print("Target distribution in train:\n", y_train.value_counts())
print("Target distribution in test:\n", y_test.value_counts())

from sklearn.preprocessing import StandardScaler

# Only scale numeric features (avoid encoded categoricals)
numeric_cols = X_train.select_dtypes(include=['float64', 'int64']).columns

scaler = StandardScaler()
X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])
X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])  # Use same scaler!



from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier()
rf.fit(X_train, y_train)
importances = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)
print(importances.head(10))

import seaborn as sns
import matplotlib.pyplot as plt

for col in numeric_cols:
    sns.boxplot(x=X_train[col])
    plt.title(f"Boxplot for {col}")
    plt.show()

from sklearn.preprocessing import StandardScaler

# Step 1: Identify numeric columns (float and int)
numeric_cols = X_train.select_dtypes(include=['float64', 'int64']).columns

# Step 2: Initialize the scaler
scaler = StandardScaler()

# Step 3: Fit the scaler ONLY on the training set to avoid data leakage
X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])

# Step 4: Transform the test set using the SAME scaler
X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])

# Optional: Check the result
print("After scaling:")
print(X_train[numeric_cols].describe().T[['mean', 'std']])

from sklearn.preprocessing import StandardScaler

# Identify numeric columns
numeric_cols = X_train.select_dtypes(include=['float64', 'int64']).columns

# Fit scaler only on numeric columns of raw X_train
scaler = StandardScaler()
X_train_scaled_numeric = scaler.fit_transform(X_train[numeric_cols])
X_test_scaled_numeric = scaler.transform(X_test[numeric_cols])

# If you have categoricals, keep them separately, and combine them back
X_train_scaled = pd.DataFrame(X_train_scaled_numeric, columns=numeric_cols, index=X_train.index)
X_test_scaled = pd.DataFrame(X_test_scaled_numeric, columns=numeric_cols, index=X_test.index)

# Combine with categoricals (if any)
categorical_cols = [col for col in X_train.columns if col not in numeric_cols]
X_train_scaled[categorical_cols] = X_train[categorical_cols]
X_test_scaled[categorical_cols] = X_test[categorical_cols]

from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
import pandas as pd

# Step 1: Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)  # Scale test set too for later use

# Step 2: Apply SMOTE
print("Before SMOTE:\n", y_train.value_counts())

smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)

print("\nAfter SMOTE:\n", pd.Series(y_train_res).value_counts())

model.fit(X_train_res, y_train_res)

numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns

for col in numeric_cols:
    plt.figure(figsize=(6, 3))
    sns.histplot(df[col], kde=True, bins=30)
    plt.title(f"Distribution of {col}")
    plt.xlabel(col)
    plt.ylabel("Count")
    plt.tight_layout()
    plt.show()

!pip install imbalanced-learn

from sklearn.dummy import DummyClassifier
from sklearn.metrics import classification_report

# Create a dummy classifier that predicts the majority class
dummy = DummyClassifier(strategy="most_frequent")
dummy.fit(X_train_res, y_train_res)

# Predict and evaluate
y_dummy_pred = dummy.predict(X_test_scaled)
print("Dummy Classifier Performance:\n")
print(classification_report(y_test, y_dummy_pred))

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Run PCA on the scaled training data
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_train_res)

# Plot PCA results
plt.figure(figsize=(8, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_train_res, cmap='coolwarm', edgecolors='k', alpha=0.7)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA Projection of Kidney Disease Dataset')
plt.colorbar(label='Class')
plt.grid(True)
plt.show()

from sklearn.ensemble import RandomForestClassifier
import pandas as pd
import numpy as np

# Train Random Forest on resampled data
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train_res, y_train_res)

# Get feature importances
importances = pd.Series(rf.feature_importances_, index=X.columns)
importances.sort_values(ascending=False).plot(kind='bar', figsize=(10, 5), title="Feature Importances")
plt.tight_layout()
plt.show()

import seaborn as sns

# Before SMOTE
sns.countplot(x=y_train, palette="pastel")
plt.title("Class Distribution Before SMOTE")
plt.show()

# After SMOTE
sns.countplot(x=y_train_res, palette="Set2")
plt.title("Class Distribution After SMOTE")
plt.show()

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)

    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("\nClassification Report:\n", classification_report(y_test, y_pred))

    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title("Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.model_selection import cross_val_score

# Step 1: Re-initialize classifiers
log_reg = LogisticRegression(max_iter=1000, random_state=42)
rf = RandomForestClassifier(random_state=42)

# Step 2: Train on SMOTE-resampled training data
log_reg.fit(X_train_res, y_train_res)
rf.fit(X_train_res, y_train_res)

# Step 3: Predict on test set
log_preds = log_reg.predict(X_test)
rf_preds = rf.predict(X_test)

# Step 4: Evaluate on test set
print("=== Logistic Regression Report ===")
print(classification_report(y_test, log_preds))

print("=== Random Forest Report ===")
print(classification_report(y_test, rf_preds))

# Step 5: Cross-validation (5-fold) for both models
log_cv_scores = cross_val_score(log_reg, X_train_res, y_train_res, cv=5, scoring='accuracy')
rf_cv_scores = cross_val_score(rf, X_train_res, y_train_res, cv=5, scoring='accuracy')

print(f"\nLogistic Regression CV Accuracy: {log_cv_scores.mean():.4f} ¬± {log_cv_scores.std():.4f}")
print(f"Random Forest CV Accuracy: {rf_cv_scores.mean():.4f} ¬± {rf_cv_scores.std():.4f}")

print("‚úÖ Current Features Used by the Model:\n")
print(X_train.columns.tolist())

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Get feature importances from your trained Random Forest
importances = rf.feature_importances_  # assumes 'rf' is your trained model
feature_names = X_train.columns  # assumes X_train is your training DataFrame

# Create DataFrame and sort
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

# Plot
plt.figure(figsize=(12, 6))
sns.barplot(data=importance_df, x='Importance', y='Feature', palette='crest')
plt.title('üîç Random Forest Feature Importance')
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
plt.tight_layout()
plt.show()

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report

# 1. Define parameter grid
param_grid = {
    'n_estimators': [50, 100, 200],          # Number of trees in forest
    'max_depth': [None, 5, 10, 20],          # Max depth of each tree
    'max_features': ['sqrt', 'log2'],        # Number of features considered per split
    'min_samples_split': [2, 5, 10],         # Minimum samples to split a node
    'min_samples_leaf': [1, 2, 4]            # Minimum samples in each leaf node
}

# 2. Initialize RandomForest
rf = RandomForestClassifier(random_state=42)

# 3. Grid Search with 5-fold CV
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,
                           cv=5, scoring='accuracy', n_jobs=-1, verbose=2)

# 4. Fit on training data
grid_search.fit(X_train_scaled, y_train)

# 5. Best estimator and score
print("‚úÖ Best Parameters:", grid_search.best_params_)
print("üéØ Best Cross-Validated Accuracy:", grid_search.best_score_)

# 6. Evaluate on test data
best_rf = grid_search.best_estimator_
test_preds = best_rf.predict(X_test_scaled)

print("\n=== Test Classification Report ===")
print(classification_report(y_test, test_preds))

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Get feature importances from the best estimator
importances = best_rf.feature_importances_

# Match feature names
feature_names = X.columns  # Assuming X is the original DataFrame before scaling

# Create DataFrame for visualization
feat_imp_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

# Display top features
print("\nüéØ Top Features by Importance:\n")
print(feat_imp_df)

# Plot
plt.figure(figsize=(12, 6))
sns.barplot(x='Importance', y='Feature', data=feat_imp_df)
plt.title('üîç Feature Importances from Tuned Random Forest')
plt.tight_layout()
plt.show()

from sklearn.inspection import permutation_importance

# Assume `best_rf` is your tuned RandomForestClassifier
result = permutation_importance(best_rf, X_test_scaled, y_test, n_repeats=30, random_state=42, n_jobs=-1)

# Convert to DataFrame
perm_df = pd.DataFrame({
    'Feature': X_test.columns,
    'Importance': result.importances_mean
}).sort_values(by='Importance', ascending=False)

# Save top features for visualization or reporting
top_features = perm_df.head(10)
print(top_features)

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Reduce to 2 principal components
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_test_scaled)

# Create DataFrame for plotting
pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])
pca_df['label'] = y_test.reset_index(drop=True)

# Plot
plt.figure(figsize=(8, 6))
plt.scatter(pca_df['PC1'], pca_df['PC2'], c=pca_df['label'], cmap='viridis', s=60)
plt.title('PCA - 2D View of Test Data')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.grid(True)
plt.show()

from sklearn.model_selection import learning_curve

train_sizes, train_scores, test_scores = learning_curve(
    best_rf, X_train_scaled, y_train, cv=5, scoring='accuracy', n_jobs=-1,
    train_sizes=np.linspace(0.1, 1.0, 5), random_state=42
)

train_mean = train_scores.mean(axis=1)
test_mean = test_scores.mean(axis=1)

plt.plot(train_sizes, train_mean, label="Training Score", marker='o')
plt.plot(train_sizes, test_mean, label="Cross-Validation Score", marker='s')
plt.title("Learning Curve")
plt.xlabel("Training Set Size")
plt.ylabel("Accuracy")
plt.legend()
plt.grid()
plt.show()

from sklearn.calibration import calibration_curve
import matplotlib.pyplot as plt

probs = best_rf.predict_proba(X_test_scaled)[:, 1]
true, pred = calibration_curve(y_test, probs, n_bins=10)

plt.figure(figsize=(6, 5))
plt.plot(pred, true, marker='o', label='Random Forest')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel("Predicted Probability")
plt.ylabel("True Probability")
plt.title("Calibration Curve")
plt.legend()
plt.grid()
plt.show()

import numpy as np
from sklearn.utils import resample
from sklearn.metrics import accuracy_score

# Bootstrapping to get CI on accuracy
n_iterations = 1000
scores = []

for _ in range(n_iterations):
    X_sample, y_sample = resample(X_test_scaled, y_test, random_state=42)
    y_pred = best_rf.predict(X_sample)
    scores.append(accuracy_score(y_sample, y_pred))

ci_lower = np.percentile(scores, 2.5)
ci_upper = np.percentile(scores, 97.5)

print(f"Accuracy 95% CI: {ci_lower:.4f} ‚Äì {ci_upper:.4f}")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Get feature importances from the best estimator
importances = best_rf.feature_importances_

# Match feature names
feature_names = X.columns  # Assuming X is the original DataFrame before scaling

# Create DataFrame for visualization
feat_imp_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

# Display top features
print("\nüéØ Top Features by Importance:\n")
print(feat_imp_df)

# Plot
plt.figure(figsize=(12, 6))
sns.barplot(x='Importance', y='Feature', data=feat_imp_df)
plt.title('üîç Feature Importances from Tuned Random Forest')
plt.tight_layout()
plt.show()

X_noisy = X_test_scaled + np.random.normal(0, 0.05, X_test_scaled.shape)
noisy_preds = best_rf.predict(X_noisy)
print("Accuracy on Noisy Test Set:", accuracy_score(y_test, noisy_preds))

import shap
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# ‚úÖ If X_test_scaled is your test data, and X has original column names
X_test_df = pd.DataFrame(X_test_scaled, columns=X.columns)

# ‚úÖ Ensure the model is fitted and available
# If not already defined, refit your best RF model like this:
# best_rf.fit(X_train_scaled, y_train)

# ‚úÖ Step 1: Initialize SHAP explainer for tree-based model
explainer = shap.Explainer(best_rf, X_test_df)

# ‚úÖ Step 2: Calculate SHAP values
shap_values = explainer(X_test_df)

# ‚úÖ Step 3: SHAP Summary Plot (global explanation for class 1)
# Select SHAP values for class 1 (index 1)
shap.plots.beeswarm(shap_values[:, :, 1]) # <-- Select values for the positive class

# ‚úÖ Step 4: Bar plot of mean absolute SHAP values
# Select SHAP values for class 1 (index 1) for the bar plot as well
shap.plots.bar(shap_values[:, :, 1]) # <-- Select values for the positive class

# ‚úÖ Step 5: SHAP Dependence Plot for most important feature
# Identify top feature based on mean absolute SHAP values for class 1
top_feat = shap_values[:, :, 1].abs.mean(0).values.argmax()
top_feat_name = X_test_df.columns[top_feat]
# Select SHAP values for class 1 (index 1) for the scatter plot
shap.plots.scatter(shap_values[:, top_feat, 1], color=shap_values[:, :, 1])

# ‚úÖ Optional: Show feature rankings
# Calculate mean absolute SHAP values for class 1
feature_importance_df = pd.DataFrame({
    "Feature": X_test_df.columns,
    "Mean |SHAP|": shap_values[:, :, 1].abs.mean(0).values # <-- Calculate mean abs SHAP for class 1
}).sort_values("Mean |SHAP|", ascending=False)

print("\nüîç Top Features by Mean |SHAP|:")
print(feature_importance_df.head(10))

from sklearn.calibration import calibration_curve
from sklearn.metrics import brier_score_loss

# Predict probabilities for the positive class
probs = best_rf.predict_proba(X_test_scaled)[:, 1]

# Brier score
brier_score = brier_score_loss(y_test, probs)
print(f"Brier Score Loss: {brier_score:.4f}")

# Calibration plot
prob_true, prob_pred = calibration_curve(y_test, probs, n_bins=10)

plt.figure(figsize=(6, 6))
plt.plot(prob_pred, prob_true, marker='o', label='Random Forest')
plt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly Calibrated')
plt.xlabel('Predicted Probability')
plt.ylabel('True Probability')
plt.title('Calibration Curve')
plt.legend()
plt.grid()
plt.show()

!pip install lime
import lime
import lime.lime_tabular
import numpy as np

explainer = lime.lime_tabular.LimeTabularExplainer(
    training_data=np.array(X_train_scaled),
    # Use the column names from the original X_train DataFrame
    feature_names=X_train.columns.tolist(),
    class_names=['No CKD', 'CKD'],
    mode='classification'
)

# Explain a single prediction
i = 43  # index of test sample
exp = explainer.explain_instance(X_test_scaled[i], best_rf.predict_proba)
exp.show_in_notebook(show_table=True)

import seaborn as sns
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt # Ensure matplotlib is imported for plt.show()

# Assuming X_test_df and shap_values are available from previous cells

# Calculate mean absolute SHAP values for each feature across all samples, specifically for class 1
# shap_values has shape (samples, features, classes)
# shap_values.values accesses the raw numpy array
# [:, :, 1] selects all samples, all features, and only the second class (index 1)
# np.abs() takes the absolute values
# .mean(axis=0) averages across the sample axis (axis 0) to get a value for each feature
mean_abs_shap_class_1 = np.abs(shap_values.values[:, :, 1]).mean(axis=0)

# Create DataFrame for SHAP Summary bar plot (top 10)
shap_df = pd.DataFrame({
    "Feature": X_test_df.columns, # This list has 23 elements
    "Mean |SHAP|": mean_abs_shap_class_1 # This list will now have 23 elements
}).sort_values(by="Mean |SHAP|", ascending=False).head(10)

plt.figure(figsize=(8, 5))
sns.barplot(data=shap_df, x="Mean |SHAP|", y="Feature", palette="coolwarm")
plt.title("Top 10 Features by Mean |SHAP| Value (Class 1)")
plt.xlabel("Mean Absolute SHAP Value")
plt.tight_layout()
plt.show()

# SHAP interaction values (for TreeExplainer only)
# First, ensure you have a SHAP TreeExplainer initialized for your model
# based on your previous SHAP code (likely in cell 149)

# Assuming 'best_rf' is your trained Random Forest model
# Assuming 'X_test_df' is your test data DataFrame (scaled)

# Re-initialize a SHAP TreeExplainer if it's not available from a previous cell
# Based on the traceback, it seems the variable 'explainer' from cell 155 is the LIME one.
# We need a SHAP TreeExplainer for interaction values.
# Let's rename the SHAP explainer to avoid confusion if you run previous cells.

# Make sure best_rf is fitted (this should be true based on prior cells)
# best_rf.fit(X_train_scaled, y_train) # Uncomment if best_rf somehow lost its state

shap_explainer_tree = shap.TreeExplainer(best_rf)

# Calculate SHAP interaction values using the SHAP explainer
# shap_interaction_values should have shape (samples, features, features, classes) for multi-output models
shap_interaction_values = shap_explainer_tree.shap_interaction_values(X_test_df)

# Summary interaction plot for class 1 (CKD class)
# We need to select the interaction values for the positive class (index 1)
# The shape should become (samples, features, features) after selecting the class.
# Then pass this sliced array to shap.summary_plot.
shap.summary_plot(shap_interaction_values[:, :, :, 1], X_test_df)

# Note: shap.summary_plot for interaction values plots the mean absolute
# interaction value between pairs of features. The output is typically
# a heatmap or a list of top interactions. The default summary_plot
# when given interaction values is often a heatmap-like representation.

import shap
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Assuming best_rf, X_test_scaled, X_train_scaled, X_train, y_train are defined from previous cells.
# Assuming X_test_df = pd.DataFrame(X_test_scaled, columns=X.columns) is available.
# Assuming shap_values is the Explanation object calculated previously (e.g., explainer(X_test_df))

# Re-run SHAP explanation if necessary, ensuring shap_values is calculated for the test set
# from cell 149:
# explainer = shap.Explainer(best_rf, X_test_df)
# shap_values = explainer(X_test_df) # This should result in shap_values with shape (80, 23, 2)

# Pass the entire shap_values object and the data X_test_df
# The summary_plot function will handle the dimensions appropriately for 'layered_violin'
# If shap_values has a class dimension (like (samples, features, classes)),
# summary_plot will usually plot for the first class or require specifying a class.
# For this dataset, class 1 (index 1) is CKD.
shap.summary_plot(shap_values[:, :, 1], X_test_df, plot_type="layered_violin")

# Explanation of the fix:
# - Original code: shap.summary_plot(shap_values[1], X_test_df, plot_type="layered_violin")
#   This passed the SHAP values for *sample index 1* (shape (features, classes)) which didn't match X_test_df (shape (samples, features)).
# - Fixed code: shap.summary_plot(shap_values[:, :, 1], X_test_df, plot_type="layered_violin")
#   This passes the SHAP values for *all samples* and *all features* specifically for *class index 1* (CKD class).
#   The shape of shap_values[:, :, 1] is (samples, features) which matches the expected input shape for the data matrix X_test_df.

import shap
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Assuming best_rf, X_test_scaled, X_train_scaled, X_train, y_train are defined from previous cells.
# Assuming X_test_df = pd.DataFrame(X_test_scaled, columns=X.columns) is available.
# Assuming shap_values is the Explanation object calculated previously (e.g., explainer(X_test_df))

# Re-run SHAP explanation if necessary, ensuring shap_values is calculated for the test set
# from cell 149:
# explainer = shap.Explainer(best_rf, X_test_df)
# shap_values = explainer(X_test_df) # This should result in shap_values with shape (80, 23, 2)

# Select the explanation for the specific sample 'i' and the positive class (index 1)
# This slicing [i, :, 1] *on the Explanation object* returns a new Explanation object
# representing the explanation for sample 'i' for the positive class.
# Pass this sliced Explanation object directly to force_plot.
shap.force_plot(shap_values[i, :, 1], matplotlib=True)

# Note: If you want to plot multiple instances in a JS plot (not matplotlib),
# you would typically pass shap_values[0:N, :, 1] for the first N samples
# to a call without matplotlib=True. For a single instance using matplotlib,
# slicing the Explanation object is cleaner.

# The base value and feature values are implicitly handled by the Explanation object.
# If you needed to provide them separately, the syntax would be closer to:
# shap.force_plot(shap_explainer_tree.expected_value[1], shap_values.values[i, :, 1], X_test_df.iloc[i], matplotlib=True)
# but the traceback suggests the first form (passing the sliced Explanation object)
# is preferred by the internal visualize function.

# Example: Index 0 of test set (you can change i to another patient index)
i = 0

# Use this for notebook visualization
shap.initjs()

# Ensure you use the SHAP explainer's expected_value, not the LIME explainer's
# Assumes 'shap_explainer_tree' was initialized in a previous cell with shap.TreeExplainer(best_rf)
# If not, re-initialize it:
# shap_explainer_tree = shap.TreeExplainer(best_rf)

# Force plot for class 1 (CKD) vs class 0 (No CKD)
# Pass the expected value for class 1 and the SHAP values for sample i and class 1
shap.force_plot(
    shap_explainer_tree.expected_value[1],  # Use the SHAP explainer's expected value for class 1
    shap_values.values[i, :, 1],          # Get the SHAP values numpy array for sample i and class 1
    X_test_df.iloc[i],                    # Pass the actual feature values for sample i
    matplotlib=True                       # Use matplotlib backend
)

import shap
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Assuming best_rf, X_test_scaled, X_train_scaled, X_train, y_train are defined from previous cells.
# Assuming X_test_df = pd.DataFrame(X_test_scaled, columns=X.columns) is available.
# Assuming shap_values is the Explanation object calculated previously (e.g., explainer(X_test_df))

# Re-run SHAP explanation if necessary, ensuring shap_values is calculated for the test set
# from cell 149:
# explainer = shap.Explainer(best_rf, X_test_df)
# shap_values = explainer(X_test_df) # This should result in shap_values with shape (80, 23, 2)

# Correct slicing: Select all samples (first ':'), all features (second ':'), and class index 1 (positive class)
# This results in a shape (80, 23), matching X_test_df shape (80, 23)
shap.summary_plot(shap_values[:, :, 1], X_test_df, plot_type="bar")

model.fit(X_train_res, y_train_res)

joblib.dump(model, 'model.pkl')
joblib.dump(scaler, 'scaler.pkl')

import os
print(os.getcwd())

from google.colab import files
files.download('model.pkl')
files.download('scaler.pkl')

!pip freeze > requirements.txt

from google.colab import files
files.download("requirements.txt")

scaler = joblib.load("scaler.pkl")
print("Features scaler was trained on:", scaler.feature_names_in_)

model = joblib.load("model.pkl")
print("Features model was trained on:", model.feature_names_in_)

st.write(f"Type of shap_values: {type(shap_values)}")
st.write(f"Shape of shap_values: {np.array(shap_values).shape}")

st.write("Predictions on first 5 samples:", model.predict(X_input[final_features].iloc[:5]))
st.write("Probabilities on first 5 samples:", model.predict_proba(X_input[final_features].iloc[:5]))

# Step 1: Install necessary libraries
!pip install streamlit shap lime joblib pandas numpy scikit-learn matplotlib seaborn

import streamlit as st
import pandas as pd
import numpy as np
import shap
import lime
import lime.lime_tabular
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.inspection import PartialDependenceDisplay
from io import StringIO
import os # Import the os module

# Load model and scaler
try:
    # Attempt to load the files from the current directory
    model = joblib.load("model.pkl")
    scaler = joblib.load("scaler.pkl")
    st.success("Model and Scaler loaded successfully.")
except FileNotFoundError:
    st.error("Error: Model or scaler files not found.")
    st.info("Please ensure 'model.pkl' and 'scaler.pkl' are in the same directory as this script.")
    st.stop() # Stop the app execution if files are missing


# Final features used in model
# NOTE: Ensure this list matches the features the model and scaler were trained on.
# The previous output from ipython-input-212 and 213 will show the feature names.
# Adjust this list if they don't match what's defined below.
# Example: scaler features_in_ might be ['age', 'bp', ..., 'hemo_bu']
# Example: model features_in_ might also be the same list.
# This list should match the columns provided to the model.
final_features = ['age', 'bp', 'sg', 'al', 'su', 'rbc', 'pc', 'pcc', 'bgr', 'bu', 'sc',
                  'sod', 'pot', 'hemo', 'wbcc', 'rbcc', 'htn', 'dm', 'appet', 'pe',
                  'bun_sc_ratio', 'high_creatinine', 'hemo_bu'] # Example list - verify this from your training code

st.title("CKD Prediction App with Explainability")
st.write("Upload your data below or manually enter values for prediction.")

# Input form
input_data = {}

# Use manual entry fields as the default unless a file is uploaded
use_manual_entry = True

uploaded_file = st.file_uploader("Upload a CSV file with required features", type=["csv"])

if uploaded_file:
    try:
        X_input_df = pd.read_csv(uploaded_file)
        st.write("Uploaded Data Preview:", X_input_df.head())
        use_manual_entry = False
    except Exception as e:
        st.error(f"Error reading uploaded file: {e}")
        st.info("Falling back to manual entry.")
        use_manual_entry = True


if use_manual_entry:
    st.subheader("Manual Input")
    # Define default values for manual entry form (optional, but good UX)
    # These should ideally be representative or based on feature distributions
    default_values = {
        'age': 45, 'bp': 80, 'sg': 1.015, 'al': 1, 'su': 0,
        'rbc': 'normal', 'pc': 'normal', 'pcc': 'notpresent', 'bgr': 150, 'bu': 50,
        'sc': 1.5, 'sod': 140, 'pot': 4.5, 'hemo': 12.0, 'wbcc': 7000,
        'rbcc': 4.5, 'htn': 'no', 'dm': 'no', 'appet': 'good', 'pe': 'no' # Add 'pe'
    }
    # Add placeholders for derived features; they will be calculated later
    default_values['bun_sc_ratio'] = default_values['bu'] / default_values['sc'] if default_values['sc'] != 0 else 0
    default_values['high_creatinine'] = 1 if default_values['sc'] > 1.2 else 0
    default_values['hemo_bu'] = default_values['hemo'] / (default_values['bu'] + 1) if default_values['bu'] >= 0 else 0


    # Input form with improved structure and default values
    with st.form("manual_input_form"):
        col1, col2, col3 = st.columns(3)
        with col1:
            input_data['age'] = st.number_input("Age", value=float(default_values.get('age', 0)))
            input_data['bp'] = st.number_input("Blood Pressure", value=float(default_values.get('bp', 0)))
            input_data['sg'] = st.selectbox("Specific Gravity", [1.005, 1.010, 1.015, 1.020, 1.025], index=[1.005, 1.010, 1.015, 1.020, 1.025].index(default_values.get('sg', 1.015)))
            input_data['al'] = st.slider("Albumin", 0, 5, value=default_values.get('al', 0))
            input_data['su'] = st.slider("Sugar", 0, 5, value=default_values.get('su', 0))
            input_data['rbc'] = st.selectbox("Red Blood Cells", ["normal", "abnormal"], index=["normal", "abnormal"].index(default_values.get('rbc', 'normal')))
            input_data['pc'] = st.selectbox("Pus Cell", ["normal", "abnormal"], index=["normal", "abnormal"].index(default_values.get('pc', 'normal')))
        with col2:
            input_data['pcc'] = st.selectbox("Pus Cell Clumps", ["present", "notpresent"], index=["present", "notpresent"].index(default_values.get('pcc', 'notpresent')))
            input_data['ba'] = st.selectbox("Bacteria", ["present", "notpresent"], index=["present", "notpresent"].index(default_values.get('ba', 'notpresent')))
            input_data['bgr'] = st.number_input("Blood Glucose Random", value=float(default_values.get('bgr', 0.0)))
            input_data['bu'] = st.number_input("Blood Urea", value=float(default_values.get('bu', 0.0)))
            input_data['sc'] = st.number_input("Serum Creatinine", value=float(default_values.get('sc', 0.0)))
            input_data['sod'] = st.number_input("Sodium", value=float(default_values.get('sod', 0.0)))
            input_data['pot'] = st.number_input("Potassium", value=float(default_values.get('pot', 0.0)))
        with col3:
            input_data['hemo'] = st.number_input("Hemoglobin", value=float(default_values.get('hemo', 0.0)))
            input_data['wbcc'] = st.number_input("White Blood Cell Count", value=float(default_values.get('wbcc', 0.0)))
            input_data['rbcc'] = st.number_input("Red Blood Cell Count", value=float(default_values.get('rbcc', 0.0)))
            input_data['htn'] = st.selectbox("Hypertension", ["yes", "no"], index=["yes", "no"].index(default_values.get('htn', 'no')))
            input_data['dm'] = st.selectbox("Diabetes Mellitus", ["yes", "no"], index=["yes", "no"].index(default_values.get('dm', 'no')))
            input_data['appet'] = st.selectbox("Appetite", ["good", "poor"], index=["good", "poor"].index(default_values.get('appet', 'good')))
            input_data['pe'] = st.selectbox("Pedal Edema", ["yes", "no"], index=["yes", "no"].index(default_values.get('pe', 'no'))) # Added 'pe'
        submit_manual = st.form_submit_button("Predict from Manual Input")

    if submit_manual:
        X_input_df = pd.DataFrame([input_data]) # Convert dictionary to DataFrame

        # Map categorical features for manual input
        mapper = {
            "normal": 0, "abnormal": 1,
            "present": 1, "notpresent": 0,
            "yes": 1, "no": 0,
            "good": 0, "poor": 1
        }
        categorical_cols_to_map = ['rbc', 'pc', 'pcc', 'ba', 'htn', 'dm', 'appet', 'pe'] # Include 'pe'
        for col in categorical_cols_to_map:
             if col in X_input_df.columns:
                X_input_df[col] = X_input_df[col].map(mapper).fillna(0) # Map and handle potential missing keys with 0

        # Ensure numerical columns are float/int
        for col in ['age', 'bp', 'sg', 'al', 'su', 'bgr', 'bu', 'sc', 'sod', 'pot', 'hemo', 'wbcc', 'rbcc']:
             if col in X_input_df.columns:
                X_input_df[col] = pd.to_numeric(X_input_df[col], errors='coerce').fillna(0) # Convert and fill errors/NaNs

        # Add derived features
        if 'bu' in X_input_df.columns and 'sc' in X_input_df.columns:
            X_input_df["bun_sc_ratio"] = X_input_df["bu"] / (X_input_df["sc"] + 1e-6) # Add small constant to avoid division by zero
        else:
             X_input_df["bun_sc_ratio"] = 0 # Default if base features missing

        if 'sc' in X_input_df.columns:
            X_input_df["high_creatinine"] = (X_input_df["sc"] > 1.2).astype(int)
        else:
            X_input_df["high_creatinine"] = 0

        if 'hemo' in X_input_df.columns and 'bu' in X_input_df.columns:
            # Note: Your previous code had hemo / (bu + 1), the original CKD notebook had hemo * bu.
            # Let's assume you meant the interaction hemo * bu as in the notebook before feature engineering.
            # If hemo / (bu+1) was intentional, keep that. Sticking to notebook's final feature list logic.
            X_input_df["hemo_bu"] = X_input_df["hemo"] * X_input_df["bu"]
        else:
            X_input_df["hemo_bu"] = 0





elif uploaded_file:
    # If a file was uploaded and read successfully, process it
    # Assume the uploaded CSV already contains the necessary features, including derived ones,
    # or that you need to add the derivation logic here for the file input as well.
    # For simplicity in this fix, let's assume the uploaded CSV is preprocessed
    # or contains the raw features that you need to process here.
    # Let's re-add the processing steps for the uploaded data too for consistency.

    # Assuming the uploaded file contains the raw features that the manual form takes
    # Re-process the uploaded dataframe
    try:
        # Map categorical features for uploaded file
        mapper = {
            "normal": 0, "abnormal": 1,
            "present": 1, "notpresent": 0,
            "yes": 1, "no": 0,
            "good": 0, "poor": 1
        }
        categorical_cols_to_map = ['rbc', 'pc', 'pcc', 'ba', 'htn', 'dm', 'appet', 'pe'] # Include 'pe'
        for col in categorical_cols_to_map:
             if col in X_input_df.columns:
                X_input_df[col] = X_input_df[col].map(mapper).fillna(0) # Map and handle potential missing keys with 0

        # Ensure numerical columns are float/int
        for col in ['age', 'bp', 'sg', 'al', 'su', 'bgr', 'bu', 'sc', 'sod', 'pot', 'hemo', 'wbcc', 'rbcc']:
             if col in X_input_df.columns:
                X_input_df[col] = pd.to_numeric(X_input_df[col], errors='coerce').fillna(0) # Convert and fill errors/NaNs

        # Add derived features
        if 'bu' in X_input_df.columns and 'sc' in X_input_df.columns:
            X_input_df["bun_sc_ratio"] = X_input_df["bu"] / (X_input_df["sc"] + 1e-6)
        else:
             X_input_df["bun_sc_ratio"] = 0

        if 'sc' in X_input_df.columns:
            X_input_df["high_creatinine"] = (X_input_df["sc"] > 1.2).astype(int)
        else:
            X_input_df["high_creatinine"] = 0

        if 'hemo' in X_input_df.columns and 'bu' in X_input_df.columns:
            X_input_df["hemo_bu"] = X_input_df["hemo"] * X_input_df["bu"]
        else:
            X_input_df["hemo_bu"] = 0



        # Ensure all final features are present
        for feature in final_features:
            if feature not in X_input_df.columns:
                X_input_df[feature] = 0

        # Reorder columns to match the training order
        X_input = X_input_df[final_features]


    except Exception as e:
        st.error(f"Error processing uploaded data for prediction: {e}")
        st.stop() # Stop if data processing fails


# Check if X_input DataFrame is ready for prediction
# It will be ready if either manual input was submitted or a file was successfully uploaded and processed
if 'X_input' in locals() and not X_input.empty:

    # Check if the number of columns matches the scaler's expectations
    if X_input.shape[1] != len(scaler.feature_names_in_):
        st.error(f"Input data has {X_input.shape[1]} features, but the scaler expects {len(scaler.feature_names_in_)} features.")
        st.write("Scaler expected features:", scaler.feature_names_in_.tolist())
        st.write("Input features received:", X_input.columns.tolist())
        st.stop()


    # Scale the input data
    try:
        X_scaled = scaler.transform(X_input)
        st.write("Scaled Data Preview:", X_scaled[:5]) # Show preview of scaled data
    except Exception as e:
        st.error(f"Error during data scaling: {e}")
        st.stop()


    # Make prediction
    try:
        prediction = model.predict(X_scaled)
        proba = model.predict_proba(X_scaled)[:, 1]

        st.subheader("Prediction")
        # Handle multiple predictions if a CSV was uploaded
        if X_input.shape[0] > 1:
            st.write("Predictions:", prediction.tolist())
            st.write("Probabilities of CKD:", [round(p, 3) for p in proba.tolist()])
            # For explanations, might want to pick one row or add a selector
            # For simplicity, explain the first row for now
            instance_to_explain_idx = 0
            st.write(f"\nShowing explanations for the first instance (Row {instance_to_explain_idx})")
            X_scaled_single = X_scaled[instance_to_explain_idx].reshape(1, -1)
            X_input_single_df = X_input.iloc[[instance_to_explain_idx]]
            prediction_single = prediction[instance_to_explain_idx]
            expected_value_single = explainer.expected_value[1] if isinstance(explainer.expected_value, list) else explainer.expected_value
            shap_vals_class1_single = shap_values[1][instance_to_explain_idx] if isinstance(shap_values, list) else shap_values[instance_to_explain_idx]


        else:
            # Single prediction from manual input
            st.write("CKD Likelihood (1 = CKD likely, 0 = CKD unlikely):", int(prediction[0]))
            st.write("Probability of CKD:", round(proba[0], 3))
            instance_to_explain_idx = 0 # Only one instance
            X_scaled_single = X_scaled
            X_input_single_df = X_input
            prediction_single = prediction[0]
            expected_value_single = explainer.expected_value[1] if isinstance(explainer.expected_value, list) else explainer.expected_value
            shap_vals_class1_single = shap_values[1][0] if isinstance(shap_values, list) else shap_values[0]


    except Exception as e:
        st.error(f"Error during prediction: {e}")
        st.stop()


    # SHAP Explanation (for the single instance selected or the first instance)
    st.subheader("üìà SHAP Explanation")
    shap.initjs()
    # Re-calculate explainer and shap_values for the dataset being predicted on
    # This is important because shap_values should correspond to X_scaled
    try:
        explainer = shap.TreeExplainer(model)
        # Calculate SHAP values for the *entire* input dataset (might be one row or many)
        shap_values_full = explainer.shap_values(X_scaled)
        # Select SHAP values for class 1 (CKD)
        shap_values_class1_full = shap_values_full[1] if isinstance(shap_values_full, list) else shap_values_full
        expected_value = explainer.expected_value[1] if isinstance(explainer.expected_value, list) else explainer.expected_value

        # Force plot for the selected instance
        st.subheader("SHAP Force Plot (Instance " + str(instance_to_explain_idx) + ")")
        # Use the single instance data X_input_single_df for feature values
        shap_html = shap.force_plot(expected_value, shap_vals_class1_single, X_input_single_df, matplotlib=False)
        from streamlit.components.v1 import html
        html(shap_html.html(), height=300)

        # SHAP Summary plot for the whole dataset (if uploaded multiple rows) or single row (if manual)
        st.subheader("üìä SHAP Summary Plot")
        fig_summary, ax = plt.subplots(figsize=(10, 6)) # Added figure size
        # Use the full SHAP values for the summary plot
        shap.summary_plot(shap_values_class1_full, X_input, plot_type="bar", show=False)
        st.pyplot(fig_summary)

    except Exception as e:
        st.error(f"Error generating SHAP plots: {e}")


    # LIME Explanation (for the single instance selected or the first instance)
    st.subheader("üü¢ LIME Explanation (Instance " + str(instance_to_explain_idx) + ")")
    try:
        # LIME explainer needs training data that was scaled
        # Ideally, use a representative sample of the *original* scaled training data
        # For this example, we'll use the current scaled input data (X_scaled) as training_data for LIME,
        # which is NOT ideal for production but works for demonstrating the fix.
        # In production, load a small sample of your actual X_train_scaled here.
        if 'X_train_scaled' not in locals() and X_scaled.shape[0] < 10:
             st.warning("LIME explainer uses the current input data as background. For better results, provide representative training data.")
             background_data_for_lime = X_scaled # Fallback for single instance
        elif 'X_train_scaled' in locals():
             background_data_for_lime = X_train_scaled # Use actual training data if available
        else:
             # If no X_train_scaled, use a sample of current X_scaled if it's large enough
             background_data_for_lime = X_scaled[:min(100, X_scaled.shape[0])] # Use up to 100 samples

        lime_explainer = lime.lime_tabular.LimeTabularExplainer(
            training_data=background_data_for_lime, # Use appropriate background data
            feature_names=final_features,
            class_names=['No CKD', 'CKD'],
            mode='classification'
        )

        # Explain the single instance's scaled data point
        lime_exp = lime_explainer.explain_instance(X_scaled_single[0], model.predict_proba, num_features=10)
        fig_lime = lime_exp.as_pyplot_figure()
        st.pyplot(fig_lime)

    except Exception as e:
        st.error(f"Error generating LIME plot: {e}")


    # Partial Dependence Plot
    st.subheader("üìê Partial Dependence Plot (PDP)")
    try:
        feature_to_plot = st.selectbox("Select feature for PDP", final_features)
        if feature_to_plot:
            fig_pdp, ax_pdp = plt.subplots()
            # PDP requires a sample of the data the model was trained on.
            # Using X_scaled (the current input) is only appropriate if it's large and representative.
            # Ideally, use a small sample of X_train_scaled here.
            # For demonstration, using X_scaled, but note this limitation.
            if 'X_train_scaled' in locals():
                 pdp_data = X_train_scaled[:min(200, X_train_scaled.shape[0])] # Use up to 200 samples from train
                 pdp_data_df = pd.DataFrame(pdp_data, columns=final_features) # Convert to DF for display function
            else:
                 pdp_data = X_scaled[:min(200, X_scaled.shape[0])] # Fallback to current input if large enough
                 pdp_data_df = pd.DataFrame(pdp_data, columns=final_features) # Convert to DF

            if feature_to_plot in pdp_data_df.columns:
                PartialDependenceDisplay.from_estimator(
                    model, pdp_data_df, features=[final_features.index(feature_to_plot)], ax=ax_pdp, feature_names=final_features
                )
                st.pyplot(fig_pdp)
            else:
                st.warning(f"Feature '{feature_to_plot}' not found in the data used for PDP.")

    except Exception as e:
        st.error(f"Error generating PDP plot: {e}")


else:
    # This block executes if no file was uploaded and manual input wasn't yet submitted
    st.info("Enter values manually or upload a CSV to get predictions and explanations.")

model.fit(X_train_res, y_train_res)

import joblib

# Assuming your final model is named `model`
# Assuming your final scaler is named `scaler`
# Assuming your final training data (scaled) is `X_train_scaled`

# If your scaled X_train isn‚Äôt already saved:
from sklearn.preprocessing import StandardScaler

# Just to confirm X_train_scaled exists; replace X_train_cleaned or X_final if needed
try:
    X_train_scaled.shape
except:
    X_train_scaled = scaler.transform(X_train)

# Save model
joblib.dump(model, "model.pkl")

# Save scaler
joblib.dump(scaler, "scaler.pkl")

# Save scaled X_train
joblib.dump(X_train_scaled, "X_train_scaled.pkl")

print("‚úÖ model.pkl, scaler.pkl, X_train_scaled.pkl saved!")

# Download them to your local machine
from google.colab import files
files.download("model.pkl")
files.download("scaler.pkl")
files.download("X_train_scaled.pkl")